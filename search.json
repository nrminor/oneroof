[
  {
    "objectID": "docs/whats-that-file.html",
    "href": "docs/whats-that-file.html",
    "title": "OneRoof Pipeline File Reference",
    "section": "",
    "text": "This document provides a comprehensive reference for all files in the OneRoof bioinformatics pipeline repository. Files are organized by directory to help you quickly find what you‚Äôre looking for."
  },
  {
    "objectID": "docs/whats-that-file.html#overview",
    "href": "docs/whats-that-file.html#overview",
    "title": "OneRoof Pipeline File Reference",
    "section": "",
    "text": "This document provides a comprehensive reference for all files in the OneRoof bioinformatics pipeline repository. Files are organized by directory to help you quickly find what you‚Äôre looking for."
  },
  {
    "objectID": "docs/whats-that-file.html#root-directory-files",
    "href": "docs/whats-that-file.html#root-directory-files",
    "title": "OneRoof Pipeline File Reference",
    "section": "Root Directory Files",
    "text": "Root Directory Files\n\nCore Pipeline Files\nmain.nf\n\nThe main entry point for the Nextflow pipeline\nOrchestrates the selection and execution of platform-specific workflows (Nanopore vs Illumina)\nHandles parameter validation and workflow routing\nEssential for running the pipeline\n\nnextflow.config\n\nCentral configuration file for the Nextflow pipeline\nDefines default parameters, process configurations, and execution profiles\nControls resource allocation, container settings, and platform-specific behaviors\nMust be understood for pipeline customization and optimization\n\n\n\nDocumentation and Configuration\nREADME.md\n\nPrimary documentation for users\nContains installation instructions, usage examples, and quick start guides\nFirst point of reference for new users\n\nCLAUDE.md\n\nAI assistant guidelines for code development\nDefines project structure, key commands, and development practices\nUseful for maintaining consistency in AI-assisted development\n\nllms.txt\n\nLLM context file\nContains project information for AI assistants\nHelps maintain consistent AI interactions\n\nLICENSE\n\nSoftware license file\nDefines terms of use and distribution\nLegal requirement for open source software\n\npyproject.toml\n\nPython package configuration and dependencies\nDefines project metadata, dependencies, and tool configurations\nEssential for Python environment setup\n\npixi.lock\n\nLock file for Pixi environment manager\nEnsures reproducible environments across different systems\nCritical for dependency management\n\njustfile\n\nTask runner configuration (similar to Makefile. but more modern, featureful, and easier to learn)\nDefines common development tasks like building Docker images and generating docs\nSpeeds up development workflow‚Äìjust run just in the same directory as the file to see what it can do\n\n\n\nEnvironment and Container Files\nContainerfile\n\nDocker/Podman container definition for the pipeline\nDefines the execution environment with all required tools\nEssential for reproducible, portable execution\n\nflake.nix & flake.lock\n\nNix package manager configuration files\nProvides an reproducible environment setup\nUseful for Nix users and HPC environments\n\nuv.lock\n\nUV package manager lock file\nExtremely fast and robust Python dependency management\nEnsures exact Python package versions across platforms, ensuring reproducibility\n\n\n\nBuild and Configuration Files\n**/_quarto.yml**\n\nQuarto documentation system configuration\nControls documentation rendering settings\nUsed for building the documentation website\n\nrefman.toml\n\nProject configuration file for our homegrown bioinformatic reference file management solution, refman\nCan be used to download batches of critical reference files for common use-cases for the pipeline\n\nnf-test.config\n\nConfiguration for Nextflow testing framework\nDefines test settings and locations\nImportant for pipeline testing and validation\n\ndata_manifest.yml\n\ndata manifest for scidataflow, a supported alternative to refman ## workflows/ Directory\n\nPlatform-specific workflow definitions that orchestrate the entire analysis pipeline:\nillumina.nf\n\nComplete workflow for processing Illumina paired-end sequencing data\nHandles FASTQ input, quality control, alignment, variant calling, and consensus generation\nOptimized for short-read sequencing characteristics\n\nnanopore.nf\n\nComplete workflow for processing Oxford Nanopore sequencing data\nSupports pod5, BAM, and FASTQ inputs with optional basecalling\nHandles long-read specific challenges and parameters"
  },
  {
    "objectID": "docs/whats-that-file.html#subworkflows-directory",
    "href": "docs/whats-that-file.html#subworkflows-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "subworkflows/ Directory",
    "text": "subworkflows/ Directory\nModular workflow components that can be reused across different main workflows:\nalignment.nf\n\nHandles read alignment to reference genomes\nIntegrates minimap2 with platform-specific parameters\nProduces sorted, indexed BAM files for downstream analysis\n\nconsensus_calling.nf\n\nGenerates consensus sequences from aligned reads\nImplements platform-specific frequency thresholds\nCritical for producing final genomic sequences\n\ngather_illumina.nf\n\nCollects and validates Illumina FASTQ files\nHandles paired-end read organization\nPrepares data for processing pipeline\n\ngather_nanopore.nf\n\nCollects Nanopore data from various formats (pod5, BAM, FASTQ)\nHandles barcode demultiplexing\nManages basecalling workflow integration\n\nhaplotyping.nf\n\nPerforms viral haplotype reconstruction\nUses Devider tool for identifying viral quasispecies\nImportant for studying viral diversity\n\nillumina_correction.nf\n\nApplies error correction specific to Illumina data\nMay include adapter trimming and quality filtering\nImproves downstream analysis accuracy\n\nmetagenomics.nf\n\nPerforms metagenomic profiling using Sylph\nIdentifies organisms present in samples\nUseful for contamination detection and co-infections\n\nphylo.nf\n\nPhylogenetic analysis using Nextclade\nAssigns sequences to clades and identifies mutations\nEssential for epidemiological tracking\n\nprimer_handling.nf\n\nManages primer validation, trimming, and analysis\nEnsures complete amplicon coverage\nCritical for amplicon sequencing workflows\n\nquality_control.nf\n\nComprehensive quality control workflow\nIntegrates FastQC, MultiQC, and custom metrics\nProduces quality reports for decision making\n\nslack_alert.nf\n\nSends notifications to Slack channels\nReports pipeline completion status\nUseful for monitoring long-running analyses\n\nvariant_calling.nf\n\nIdentifies genetic variants from aligned reads\nUses ivar for amplicon data, bcftools for general data\nProduces VCF files for downstream analysis"
  },
  {
    "objectID": "docs/whats-that-file.html#modules-directory",
    "href": "docs/whats-that-file.html#modules-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "modules/ Directory",
    "text": "modules/ Directory\nIndividual process definitions for specific bioinformatics tools:\n\nBasecalling and Preprocessing\ndorado.nf\n\nOxford Nanopore basecaller integration\nConverts pod5 files to FASTQ with quality scores\nRequires GPU for optimal performance\n\nchopper.nf\n\nQuality filtering for long reads\nRemoves low-quality Nanopore sequences\nImproves downstream analysis quality\n\nfastp.nf\n\nFast preprocessing for Illumina reads\nPerforms quality filtering and adapter trimming\nGenerates QC reports\n\ncutadapt.nf\n\nAdapter and primer trimming tool\nRemoves sequencing artifacts\nEssential for accurate variant calling\n\n\n\nAlignment and Coverage\nminimap2.nf\n\nVersatile sequence aligner\nHandles both short and long reads\nPrimary alignment tool in the pipeline\n\nsamtools.nf\n\nSAM/BAM file manipulation\nSorting, indexing, and filtering alignments\nEssential for BAM file processing\n\nmosdepth.nf\n\nFast coverage depth calculation\nGenerates coverage statistics and plots\nImportant for quality assessment\n\ncramino.nf\n\nCRAM/BAM file statistics\nProvides quick alignment metrics\nUseful for QC checks\n\n\n\nVariant Calling and Consensus\nivar.nf\n\nVariant calling and consensus for amplicon data\nHandles primer trimming and frequency-based calling\nPrimary tool for viral genomics\n\nbcftools.nf\n\nGeneral-purpose variant calling and manipulation\nVCF file processing and filtering\nComplementary to ivar for specific tasks\n\nsnpeff.nf\n\nVariant annotation tool\nPredicts functional effects of variants\nImportant for biological interpretation\n\n\n\nQuality Control and Reporting\nfastqc.nf\n\nSequence quality control\nGenerates detailed quality metrics\nStandard tool for NGS QC\n\nmultiqc.nf\n\nAggregates QC reports from multiple tools\nCreates unified quality report\nEssential for multi-sample projects\n\nplot_coverage.nf\n\nCustom coverage visualization\nCreates coverage plots per amplicon\nHelps identify coverage gaps\n\nreporting.nf\n\nGenerates analysis reports\nCompiles results into readable formats\nUser-facing output generation\n\n\n\nSpecialized Tools\nnextclade.nf\n\nViral clade assignment and phylogenetics\nIdentifies mutations and QC issues\nEssential for SARS-CoV-2 and influenza analysis\n\nsylph.nf\n\nMetagenomic profiling\nFast organism identification\nUseful for contamination detection\n\ndevider.nf\n\nViral haplotype reconstruction\nIdentifies quasispecies in samples\nImportant for studying viral diversity\n\namplicon-tk.nf\n\nAmplicon analysis toolkit\nWill provide amplicon-specific utilities\nSupports targeted sequencing workflows\nMay be used for contamination detection\n\n\n\nUtility Modules\nbedtools.nf\n\nBED file manipulation\nGenomic interval operations\nUsed for primer and region handling\n\nseqkit.nf\n\nSequence manipulation toolkit\nFASTA/FASTQ processing utilities\nGeneral sequence handling\n\nrasusa.nf\n\nRead subsampling tool\nReduces coverage to specified depth\nHelps manage computational resources\n\nvsearch.nf\n\nSequence clustering and searching\nSupports sequence similarity analyses\n\nduckdb.nf\n\nSQL database for data analysis\nEnables complex data queries\ncurrently not implemented in the pipeline\n\ngrepq.nf\n\nPattern matching in sequences\nQuick sequence searching\nUtility for sequence filtering\ncurrently not implemented in the pipeline\n\nbbmap.nf\n\nBBMap tool suite integration\nVarious sequence processing utilities\nAlternative/complementary to other tools\n\ndeacon.nf\n\ncustomizable decontamination module\ncurrently not implemented in the pipeline\n\n\n\nPipeline-Specific Modules\nvalidate.nf\n\nInput validation module\nChecks file formats and parameters\nEnsures pipeline requirements are met\n\nprimer_patterns.nf\n\nGenerates primer search patterns\nSupports primer identification in reads\nImportant for primer trimming\n\nsplit_primer_combos.nf\n\nSplits primers by combinations\nHandles complex primer schemes\nSupports multiplexed amplicons\n\nresplice_primers.nf\n\nRe-splices primer sequences\nMay handle primer artifacts\nSpecialized primer processing\n\nwrite_primer_fasta.nf\n\nOutputs primers in FASTA format\nUtility for primer sequence export\nSupports downstream analyses\n\noutput_primer_tsv.nf\n\nExports primer information as TSV\nCreates tabular primer summaries\nUseful for documentation\n\nconcat_consensus.nf\n\nConcatenates consensus sequences\nCombines multi-segment genomes\nImportant for segmented viruses\n\nfile_watcher.nf\n\nMonitors directories for new files\nEnables real-time processing\nSupports continuous sequencing runs\n\ncall_slack_alert.nf\n\nSends Slack notifications\nReports pipeline events\nPart of monitoring system"
  },
  {
    "objectID": "docs/whats-that-file.html#bin-directory",
    "href": "docs/whats-that-file.html#bin-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "bin/ Directory",
    "text": "bin/ Directory\nPython scripts and utilities for data processing:\n\nCore Analysis Scripts\nivar_variants_to_vcf.py\n\nConverts ivar variant output to standard VCF format\nFixes known issues with ivar‚Äôs VCF generation\nEssential for variant calling pipeline\n\nplot_coverage.py\n\nGenerates coverage plots from alignment data\nCreates visual representation of sequencing depth\nHelps identify problematic regions\n\nconcat_consensus.py\n\nConcatenates consensus sequences from multiple segments\nHandles multi-segment viruses like influenza\nProduces complete genome sequences\n\ngenerate_variant_pivot.py\n\nCreates pivot tables of variants across samples\nUseful for comparing mutations between samples\nSupports epidemiological analyses\n\n\n\nPrimer Management Scripts\nvalidate_primer_bed.py\n\nValidates primer BED file format and content\nChecks for primer pair completeness\nPrevents primer-related pipeline failures\n\nmake_primer_patterns.py\n\nGenerates regex patterns for primer detection\nHandles primer orientation and mismatches\nSupports primer trimming accuracy\n\nsplit_primer_combos.py\n\nSeparates primers by pool/combination\nHandles multiplexed primer schemes\nImportant for complex protocols\n\nresplice_primers.py\n\nPython implementation of primer resplicing\nHandles primer artifacts in sequences\nComplements Rust version\n\nresplice_primers.rs\n\nRust implementation for performance\nFast primer sequence processing\nUsed in high-throughput scenarios\n\n\n\nMonitoring and Utilities\nfile_watcher.py\n\nMonitors directories for new sequencing files\nTriggers pipeline execution automatically\nEnables real-time analysis\n\nslack_alerts.py\n\nSends notifications to Slack\nReports pipeline status and errors\nIntegrated with monitoring workflow\n\nmultisample_plot.py\n\nCreates plots comparing multiple samples\nVisualizes cross-sample metrics\nUseful for batch analysis\n\n\n\nPackage Files\ninit.py\n\nPython package initialization\nMakes bin/ directory a Python module\nEnables script imports\n\nmain.py\n\nPackage entry point\nAllows running as python -m bin\nMay provide CLI interface\n\n\n\nTest Files\n**test_*.py files**\n\nUnit tests for corresponding scripts\nEnsures script functionality\nPart of quality assurance"
  },
  {
    "objectID": "docs/whats-that-file.html#conf-directory",
    "href": "docs/whats-that-file.html#conf-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "conf/ Directory",
    "text": "conf/ Directory\nConfiguration files for various pipeline components:\nnanopore.config\n\nNanopore-specific pipeline settings\nDefines basecalling models, parameters\nOptimizes for long-read characteristics\n\nillumina.config\n\nIllumina-specific pipeline settings\nShort-read optimized parameters\nHandles paired-end specific options\n\nsnpeff.config\n\nSnpEff variant annotation settings\nDefines reference databases\nControls annotation behavior\n\nfile_watcher.template.yml\n\nTemplate for file watcher configuration\nDefines monitoring parameters\nCustomizable for different setups"
  },
  {
    "objectID": "docs/whats-that-file.html#lib-directory",
    "href": "docs/whats-that-file.html#lib-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "lib/ Directory",
    "text": "lib/ Directory\nGroovy libraries for Nextflow:\nUtils.groovy\n\nUtility functions for Nextflow workflows\nCommon functionality across workflows\nReduces code duplication"
  },
  {
    "objectID": "docs/whats-that-file.html#docs-directory",
    "href": "docs/whats-that-file.html#docs-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "docs/ Directory",
    "text": "docs/ Directory\nProject documentation sources:\n\nCore Documentation\nindex.qmd\n\nMain documentation page source\nRenders to HTML/PDF documentation\nUser-facing pipeline guide\n\ndeveloper.qmd & developer.md\n\nDeveloper documentation\nTechnical details for contributors\nCode structure and patterns\n\npipeline_architecture.qmd & pipeline_architecture.md\n\nDetailed pipeline design documentation\nArchitectural decisions and flow\nTechnical reference\n\ndata_management.qmd & data_management.md\n\nData handling guidelines\nStorage and organization practices\nBest practices documentation\n\n\n\nGenerated Files\npipeline_architecture_files/\n\nQuarto-generated web assets\nJavaScript, CSS, and fonts\nSupports interactive documentation"
  },
  {
    "objectID": "docs/whats-that-file.html#globus-directory",
    "href": "docs/whats-that-file.html#globus-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "globus/ Directory",
    "text": "globus/ Directory\nGlobus integration for data transfer:\nREADME.md\n\nGlobus setup instructions\nConfiguration guidelines\nIntegration documentation\n\naction_provider/\n\nGlobus action provider implementation\nEnables automated workflows\nCloud integration support\n\nconfig/\n\nGlobus configuration files\nService settings\nAuthentication setup\n\nflows/\n\nGlobus flow definitions\nAutomated data workflows\nPipeline integration\n\nscripts/\n\nDeployment and testing scripts\nGlobus service management\nOperational utilities"
  },
  {
    "objectID": "docs/whats-that-file.html#tests-directory",
    "href": "docs/whats-that-file.html#tests-directory",
    "title": "OneRoof Pipeline File Reference",
    "section": "tests/ Directory",
    "text": "tests/ Directory\nTest files and data:\nREADME.md\n\nTest documentation\nRunning test instructions\nTest data descriptions\n\ndata/\n\nTest datasets\nExample files for each data type\nValidation datasets\n\nmodules/, subworkflows/, workflows/\n\nNextflow test definitions\nUnit and integration tests\nPipeline validation"
  },
  {
    "objectID": "docs/whats-that-file.html#github-workflows-.github",
    "href": "docs/whats-that-file.html#github-workflows-.github",
    "title": "OneRoof Pipeline File Reference",
    "section": "GitHub Workflows (.github/)",
    "text": "GitHub Workflows (.github/)\nworkflows/test.yml\n\nCI/CD test workflow\nAutomated testing on commits\nQuality assurance\n\nworkflows/docker-image.yaml\n\nDocker image building workflow\nAutomated container updates\nDeployment automation"
  },
  {
    "objectID": "docs/whats-that-file.html#summary",
    "href": "docs/whats-that-file.html#summary",
    "title": "OneRoof Pipeline File Reference",
    "section": "Summary",
    "text": "Summary\nThe OneRoof pipeline repository is organized into logical directories that separate:\n\nCore pipeline logic (workflows/, subworkflows/, modules/)\nUtility scripts (bin/)\nConfiguration (conf/, *.config)\nDocumentation (docs/, *.md)\nTest infrastructure (tests/)\nReference data (assets/)\nExternal integrations (globus/)\n\nThis structure promotes modularity, reusability, and maintainability while supporting both Nanopore and Illumina sequencing platforms for viral genomics applications."
  },
  {
    "objectID": "docs/index.html#overview",
    "href": "docs/index.html#overview",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Overview",
    "text": "Overview\noneroof is a Nextflow pipeline designed to take a common series of bioinformatic tasks (see below) and put them under ‚Äúone roof‚Äù. We mean this literally as well as figuratively: the pipeline will perform certain tasks best when run on networked devices in the same building.\noneroof was originally developed in the early stages of the United States Bovine Highly Pathogenic Avian Influenza (HPAI) outbreak of 2024, when we wanted one, configurable, easy-to-run pipeline that would do all of the following seamlessly:\n\nHandle super-accuracy basecalling with GPU acceleration on pod5-formatted Nanopore signal files, working on GCP, AWS, or Slurm if need be.\nDemultiplex BAM-formatted reads that come out of basecalling.\nPerform the above two steps as signal files become available, either locally or remotely via an SSH client.\nAccept raw read BAMs or FASTQs if basecalling and demultiplexing have already been performed elsewhere.\nAccept paired Illumina reads in addition to Nanopore reads.\nUse forward and reverse primer sequences to select only those reads that represent complete amplicons.\nTrim away primers‚Äîand also any bases that are upstream of the forward primer or downstream of the reverse primer, while only retaining those reads identified as complete amplicons by containing both primers.\nAlign to a custom reference with the proper presets for the provided data.\nCall variants and consensus sequences with appropriate settings for the provided data.\nPerform tree building with nextclade, quality introspection with multiQC, and error correction based on the input sequence platform.\n\nThough many excellent pipelines currently exist, e.g.¬†nf-core/viralrecon, epi2me-labs/wf-amplicon, and nf-core/nanoseq, none of these pipelines quite handled all of the above. oneroof seeks to handle these requirements while remaining highly configurable for users, highly modular for developers, and easy to control in the command line for both.\nOverall, oneroof can be summarized as a variant-calling pipeline written in and managed by Nextflow. Its software dependencies are provided through containers or through an environment assembled by pixi. To run it on your own Nanopore pod5s with Docker containers, simply run something like:\nnextflow run nrminor/oneroof \\\n--pod5_dir my_pod5_dir \\\n--primer_bed my_primers.bed \\\n--refseq my_ref.fasta \\\n--ref_gbk my_ref.gbk \\\n--kit \"SQK-NBD114-24\"\nThese are the core elements required to run on Nanopore data: a directory of pod5 files, a BED file of primer coordinates, a reference sequence in FASTA and Genbank format, and the Nanopore barcoding kit used.\nAnd for Illumina paired-end reads, it‚Äôs even simpler:\nnextflow run nrminor/oneroof \\\n--illumina_fastq_dir my_illumina_reads/\nIf you want to use Apptainer containers instead of Docker, just add -profile apptainer to either of the above nextflow run commands. And if you don‚Äôt want to use containers at all, simply run pixi shell --frozen to bring all the pipeline‚Äôs dependencies into scope and then add -profile containerless to your nextflow run command.\nNextflow pipelines like this one have a ton of configuration, which can be overwhelming for beginners and new users. To make this process easier, we‚Äôre developing a Terminal User Interface (TUI) to guide you through setup. Please stay tuned!"
  },
  {
    "objectID": "docs/index.html#quick-start",
    "href": "docs/index.html#quick-start",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Quick Start",
    "text": "Quick Start\nFor most users, oneroof will have two core requirements: The Docker container engine, available here, and Nextflow, available here. For users interested in super-accuracy basecalling Nanopore signal files, an on-board GPU supported by the Dorado basecaller is also required.\nAll remaining software dependencies will be supplied through the pipeline‚Äôs Docker image, which will be pulled and used to launch containers automatically.\nFrom there, the pipeline‚Äôs three data dependencies are sequence data in BAM, FASTQ, or POD5 format, a BED file of primer coordinates, and a reference sequence in FASTA and Genbank format. For Nanopore data, a barcoding kit identifier is also required. Simply plug in these files to a command like the above and hit enter!"
  },
  {
    "objectID": "docs/index.html#detailed-setup-instructions",
    "href": "docs/index.html#detailed-setup-instructions",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Detailed Setup Instructions",
    "text": "Detailed Setup Instructions\n\nConfiguration\nMost users should configure oneroof through the command line via the following parameters:\n\n\n\nCommand line argument\nDefault value\nExplanation\n\n\n\n\n--primer_bed\nNone\nA bed file of primer coordinates relative to the reference provided with the parameters refseq and ref_gbk.\n\n\n--refseq\nNone\nThe reference sequence to be used for mapping in FASTA format.\n\n\n--ref_gbk\nNone\nThe reference sequence to be used for variant annotation in Genbank format.\n\n\n--remote_pod5_location\nNone\nA remote location to use with an SSH client to watch for pod5 files in realtime as they are generated by the sequencing instrument.\n\n\n--file_watcher_config\nNone\nConfiguration file for remote file monitoring. An example can be found at conf/file_watcher.template.yml.\n\n\n--pod5_dir\nNone\nIf a remote pod5 location isn‚Äôt given, users may provide a local, on-device directory where pod5 files have been manually transferred.\n\n\n--precalled_staging\nNone\nA local directory to watch for Nanopore FASTQs or BAMs as the become available. This is for cases where basecalling is being performed via another workflow, resulting in BAMs or FASTQs that are gradually transferred into params.precalled_staging as basecalling completes.\n\n\n--prepped_data\nNone\nIf pod5 files have already been basecalled, demultiplexed, and transferred to a local directory accessible to onerood users can specify their location with --prepped_data.\n\n\n--illumina_fastq_dir\nNone\nIf users have Illumina data to be processed, they may specify their paired-end FASTQ files‚Äô location with illumina_fastq_dir.\n\n\n--model\nsup@latest\nthe Nanopore basecalling model to apply to the provided pod5 data (defaults to the latest super-accuracy version)\n\n\n--kit\nNone\nThe Nanopore barcoding kit used to prepare sequencing libraries.\n\n\n--pod5_batch_size\nall pod5s\nHow many pod5 files to basecall at once. With a single available GPU, all pod5 files should be basecalled together, so this parameter defaults to telling Nextflow to take all pod5 files at once.\n\n\n--basecall_max\n1\nIf basecalling pod5 files is to be parallelized across multiple available GPUs, this parameter tells Nextflow how many parallel instances of the basecaller to run at once (defaults to 1).\n\n\n--max_len\n12345678910\nThe maximum acceptable length for a given read.\n\n\n--min_len\n1\nThe minimum acceptable length for a given read.\n\n\n--min_qual\n0\nThe minimum acceptable average quality for a given read.\n\n\n--max_mismatch\n0\nThe maximum number of mismatches to allow when finding primers\n\n\n--downsample_to\n0\nDesired coverage to downsample to, with a special value of 0 to indicate 0 downsampling\n\n\n--secondary\nNone\nWhether to turn on secondary alignments for each amplicon. Secondary alignments can increase depth at the cost of more reads potentially mapping to the wrong locations. By default, secondary alignments are off.\n\n\n--min_consensus_freq\n0.5\nThe minimum required frequency of a variant base to be included in a consensu sequence.\n\n\n--min_depth_coverage\n20\nMinimum required depth of coverage to call a variant.\n\n\n--min_variant_frequency\n0.05 (illumina) or 0.10 (nanopore)\nMinimum variant frequency to call a variant.\n\n\n--meta_ref\nNone\nDataset, either a local FASTA file or a pre-built dataset built by Sylph, to use for metagenomic profiling. Can download prebuilt ones here: Pre-built Sylph Databases.\n\n\n--sylph_tax_db\nNone\nThe taxonomic annotation for the sylph database specified with --meta_ref. The pipeline automaticially downloads the databases so only the identifier is needed here.\n\n\n--nextclade_dataset\nNone\nThe name of the dataset to run nextclade with. To see all dataset options run nextclade dataset list --only-names.\n\n\n--results\nresults/\nWhere to place results.\n\n\n--cleanup\nfalse\nWhether to cleanup work directory after a successful run.\n\n\n\nNote that oneroof checks for how to gather data in a particular order for Nanopore data, which mirrors the order they are listed in the table above:\n\nFirst, oneroof checks if the user has supplied a remote location with --remote_pod5_location, at which point it will launch the file watcher module and begin transferring and basecalling batches of pod5 files. The size of these batches can be controlled with --pod5_batch_size, which defaults to basecalling batches of 100 pod5s.\nIf no remote directory is provided, oneroof checks for a local pod5 directory from --pod5_dir.\nIf the user doesn‚Äôt provide a local --pod5_dir, oneroof assumes that pre-basecalled BAMs or FASTQs are being provided. These can either be watched for with the directory from --precalled_staging, or run immediately with the directory from --prepped_data.\n\n\n\nDeveloper Setup\noneroof depends on software packages supplied through various conda registries as well as through PyPI, the Python Package Index. To unify these various channels, we used the relatively new pixi package and environment manager, which stores all dependencies from both locations in the file pyproject.toml.\nTo reproduce the environment required by this pipeline, make sure you are on a Mac, a linux machine, or a Windows machine using Windows Subsystem for Linux. Then, to reproduce the environment, install pixi with:\nPIXI_ARCH=x86_64 curl -fsSL https://pixi.sh/install.sh | bash\nDownload the pipeline with:\ngit clone https://github.com/nrminor/oneroof.git && cd oneroof\nAnd then open a pixi subshell within your terminal with:\npixi shell --frozen\nAs long as you are using a supported system, the pipeline should run within that subshell. You can also run the pipeline within that subshell without containers using the ‚Äúcontainerless‚Äù profile:\nnextflow run . \\\n-profile containerless \\\n--pod5_dir my_pod5_dir \\\n--primer_bed my_primers.bed \\\n--refseq my_ref.fasta \\\n--ref_gbk my_ref.gbk \\\n--kit \"SQK-NBD114-24\"\nEspecially on Apple Silicon Macs, this will reduce the overhead of using the Docker Virtual Machine and allow the pipeline to invoke tools installed directly within the local project environment.\nNote also that more information on the repo‚Äôs files is available in our developer guide."
  },
  {
    "objectID": "docs/index.html#testing",
    "href": "docs/index.html#testing",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Testing",
    "text": "Testing\nOneRoof includes a comprehensive test suite built with nf-test, the official testing framework for Nextflow pipelines. The test suite validates pipeline functionality through module, workflow, and end-to-end tests.\n\nRunning Tests\nBefore contributing or deploying changes, ensure all tests pass:\n# Ensure you're in the Pixi environment (includes nf-test)\npixi shell --frozen\n\n# Run all tests\njust test\n\n# Run tests with verbose output\njust test-verbose\n\n# Run specific test categories\njust test-modules      # Test individual processes\njust test-workflows    # Test platform workflows\njust test-pipeline     # Test end-to-end functionality\n\n# Run a specific test file\njust test-file tests/modules/minimap2_align.nf.test\n\n# Update test snapshots after intentional changes\njust test-update\n\n\nTest Structure\nTests are organized in the tests/ directory: - tests/modules/ - Unit tests for individual processes - tests/workflows/ - Integration tests for platform-specific workflows - tests/pipelines/ - End-to-end pipeline tests - tests/data/ - Minimal test datasets\nThe test suite uses minimal synthetic data to ensure fast execution while still exercising all key pipeline features. Tests are automatically run in CI/CD on all pull requests and pushes to main branches.\nFor more details on the testing framework and how to write new tests, see the test suite documentation."
  },
  {
    "objectID": "docs/index.html#further-documentation",
    "href": "docs/index.html#further-documentation",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Further Documentation",
    "text": "Further Documentation\nFor more detailed information about specific aspects of the OneRoof pipeline, please refer to the following documentation:\n\nDeveloper Guide - Comprehensive guide for developers working with the codebase, including project structure, coding standards, and development workflows\nPipeline Architecture - Detailed technical documentation of the pipeline‚Äôs architecture, including workflow diagrams, module descriptions, and data flow\nFile Reference Guide - Complete listing of all files in the repository with descriptions of their purpose and functionality\nGlobus Integration - Instructions for setting up and using Globus integration for automated data transfer and remote workflow execution\nTest Suite Documentation - Guide to the testing framework, including how to write and run tests, test data organization, and CI/CD integration"
  },
  {
    "objectID": "docs/index.html#contributing",
    "href": "docs/index.html#contributing",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Contributing",
    "text": "Contributing\nContributions, feature requests, improvement suggestions, and bug reports via GitHub issues are all welcome! For more information on how to work with the project and what all the repo files are, see our developer guide."
  },
  {
    "objectID": "docs/index.html#is-it-any-good",
    "href": "docs/index.html#is-it-any-good",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Is it any good?",
    "text": "Is it any good?\nYes."
  },
  {
    "objectID": "docs/index.html#citation",
    "href": "docs/index.html#citation",
    "title": "OneRoof: Base-, Variant-, and Consensus-calling under One Proverbial Roof.",
    "section": "Citation",
    "text": "Citation\n\nLail, Andrew J., William C. Vuyk, Heather Machkovech, Nicholas R. Minor, Nura R. Hassan, Rhea Dalvie, Isla E. Emmen et al.¬†‚ÄúAmplicon sequencing of pasteurized retail dairy enables genomic surveillance of H5N1 avian influenza virus in United States cattle.‚Äù PloS one 20, no. 6 (2025): https://doi.org/10.1371/journal.pone.0325203.\n\n@article{Lail2025-xf,\n  title = \"Amplicon sequencing of pasteurized retail dairy enables genomic surveillance of {H5N1} avian influenza virus in United States cattle\",\n  author = \"Lail, Andrew J and Vuyk, William C and Machkovech, Heather and Minor, Nicholas R and Hassan, Nura R and Dalvie, Rhea and Emmen, Isla E and Wolf, Sydney and Kalweit, Annabelle and Wilson, Nancy and Newman, Christina M and Tiburcio, Patrick Barros and Weiler, Andrea and Friedrich, Thomas C and O'Connor, David H\",\n  journal = \"PLoS One\",\n  publisher = \"Public Library of Science (PLoS)\",\n  volume =  20,\n  number =  6,\n  pages = \"e0325203\",\n  month =  jun,\n  year =  2025,\n  copyright = \"http://creativecommons.org/licenses/by/4.0/\",\n  language = \"en\"\n}\n\nKwon, Taeyong, Jessie D. Trujillo, Mariano Carossino, Heather M. Machkovech, Konner Cool, Eu Lim Lyoo, Gagandeep Singh et al.¬†‚ÄúPathogenicity and transmissibility of bovine-derived HPAI H5N1 B3. 13 virus in pigs.‚Äù Emerging Microbes & Infections just-accepted (2025): https://doi.org/10.1080/22221751.2025.2509742.\n\n@ARTICLE{Kwon2025-yq,\n  title = \"Pathogenicity and transmissibility of bovine-derived {HPAI} {H5N1} B3.13 virus in pigs\",\n  author = \"Kwon, Taeyong and Trujillo, Jessie D and Carossino, Mariano and Machkovech, Heather M and Cool, Konner and Lyoo, Eu Lim and Singh, Gagandeep and Kafle, Sujan and Elango, Shanmugasundaram and Vediyappan, Govindsamy and Wei, Wanting and Minor, Nicholas and Matias-Ferreyra, Franco S and Morozov, Igor and Gaudreault, Natasha N and Balasuriya, Udeni B R and Hensley, Lisa E and Diel, Diego G and Ma, Wenjun and Friedrich, Thomas C and Richt, Juergen A\",\n  journal = \"Emerg. Microbes Infect.\",\n  publisher = \"Informa UK Limited\",\n  volume =  14,\n  number =  1,\n  pages = \"2509742\",\n  month =  dec,\n  year =  2025,\n  keywords = \"Cattle; H5N1 genotype B3.13; highly pathogenic avian influenza; mammalian-like mutation; pathogenicity; pig; transmissibility\",\n  copyright = \"http://creativecommons.org/licenses/by-nc/4.0/\",\n  language = \"en\"\n}"
  },
  {
    "objectID": "docs/data_management.html",
    "href": "docs/data_management.html",
    "title": "Reproducible Data Asset Management for Oneroof",
    "section": "",
    "text": "Coming soon! This tutorial will cover managing data assets reproducibly with either of two command-line tools: SciDataFlow and refman.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/developer.html",
    "href": "docs/developer.html",
    "title": "Developer Guide",
    "section": "",
    "text": "Most oneroof development happens in the dev branch of this repo. To get started with doing so yourself, clone and switch to the dev branch like so:\ngit clone https://github.com/nrminor/oneroof.git && \\\ncd oneroof && \\\ngit checkout dev\nFrom there, you‚Äôll have a few tools to consider installing if you haven‚Äôt already:\n\nThe Pixi package manager, available here: https://pixi.sh/latest/\nThe command runner Just, available here: https://just.systems/man/en/chapter_4.html\nPre-commit, available here: https://pre-commit.com/\nDocker to use the repo Dockerfile as a dev container, available here: https://docs.docker.com/engine/install/\nQuarto to modify documentation, available here: https://quarto.org/docs/get-started/\nAn editor with language support for Nextflow (or Groovy as a fallback), Python, and Quarto (or Markdown as a fallback).\n\nMuch of this project was written in VSCode, as it‚Äôs currently the editor that makes it easiest to install Nextflow, Quarto, and Python language servers. We recommend developers use VSCode (or one of the VSCode forks, e.g., Cursor, Positron, etc.) for the same reason, though editors like Zed, Helix, or NeoVim can also be set up to provide the same functionality.\n\n\nOf the above tools, Pixi is the only one that is essential for development, as it handles installing the pipeline‚Äôs dependencies from various conda registries as well as the Python Package Index. That said, you could get away with not using pixi by manually installing the packages in pyproject.toml in a conda environment, using pip when needed. While this solution is inelegant compared to having a unified package manager like Pixi, it may be more familiar to some users.\nIf you are sticking with Pixi, run pixi shell --frozen in the project root before you get started. This will create a terminal environment with everything you need to work on and run oneroof (or rather: everything except Dorado. More on that below). The --frozen flag is critical; it tells Pixi to install the exact dependency versions recorded in pixi.lock as opposed to searching for newer versions where possible. This ensures reproducibility, as updates to the dependencies in pixi.lock will only be pushed when they have been tested on Linux and MacOS systems by the core maintainer team.\n\n\n\nWhile Just isn‚Äôt as important as Pixi, I would still recommend installing it because of the conveniences it offers. With Just, the repo justfile provides a switchboard of command shorthands, including:\n\njust docs, which runs a series of Quarto commands to render and bundle the repo docs (including this file) as well as construct an updated readme\njust py, which lints and formats all the repo‚Äôs Python files.\njust docker, which builds and pushes a new version of the repo Docker Image.\njust env, which instantiates the Pixi environment.\njust all, which does everything (just doit will do the same thingüòé).\n\nRun just in the same directory as the repo justfile to list all available recipes, and check out the Just Programmer‚Äôs Manual for more about Just.\n\n\n\nIf you run pre-commit install in the repo root directory, it will install the pre-commit hooks in our .pre-commit-config.yaml. These hooks make sure that formatting throughout the repo files are consistent before they can be committed. Again, not essential, but nice!\n\n\n\nIf you don‚Äôt want to futz with all of the above or with setting up Dorado locally, you can also use the pipeline‚Äôs Docker Hub image nrminor/dorado-and-friends:v0.2.3 as a dev container. It should bring everything the pipeline and its dev environment needs, though its use as a dev container has not yet been tested.\n\n\n\nQuarto can be thought of as a renderer or compiler for documents written in supercharged Markdown. It can run code blocks, render to HTML, PDF, reveal.js presentations, Microsoft Powerpoint and Word documents, websites, books, and dozens of other formats. As such, I use Quarto markdown documents as a sort of ‚Äúur-format,‚Äù and render it out to other formats as desired. To render the project readme and any other .qmd documents in this project, you will need Quarto installed. The Quarto config file, _quarto.yml, controls project level settings, including a post-render section telling it to regenerate the Github-markdown-formatted readme from assets/index.qmd (via the project‚Äôs just readme recipe)."
  },
  {
    "objectID": "docs/developer.html#oneroof-development-environment",
    "href": "docs/developer.html#oneroof-development-environment",
    "title": "Developer Guide",
    "section": "",
    "text": "Most oneroof development happens in the dev branch of this repo. To get started with doing so yourself, clone and switch to the dev branch like so:\ngit clone https://github.com/nrminor/oneroof.git && \\\ncd oneroof && \\\ngit checkout dev\nFrom there, you‚Äôll have a few tools to consider installing if you haven‚Äôt already:\n\nThe Pixi package manager, available here: https://pixi.sh/latest/\nThe command runner Just, available here: https://just.systems/man/en/chapter_4.html\nPre-commit, available here: https://pre-commit.com/\nDocker to use the repo Dockerfile as a dev container, available here: https://docs.docker.com/engine/install/\nQuarto to modify documentation, available here: https://quarto.org/docs/get-started/\nAn editor with language support for Nextflow (or Groovy as a fallback), Python, and Quarto (or Markdown as a fallback).\n\nMuch of this project was written in VSCode, as it‚Äôs currently the editor that makes it easiest to install Nextflow, Quarto, and Python language servers. We recommend developers use VSCode (or one of the VSCode forks, e.g., Cursor, Positron, etc.) for the same reason, though editors like Zed, Helix, or NeoVim can also be set up to provide the same functionality.\n\n\nOf the above tools, Pixi is the only one that is essential for development, as it handles installing the pipeline‚Äôs dependencies from various conda registries as well as the Python Package Index. That said, you could get away with not using pixi by manually installing the packages in pyproject.toml in a conda environment, using pip when needed. While this solution is inelegant compared to having a unified package manager like Pixi, it may be more familiar to some users.\nIf you are sticking with Pixi, run pixi shell --frozen in the project root before you get started. This will create a terminal environment with everything you need to work on and run oneroof (or rather: everything except Dorado. More on that below). The --frozen flag is critical; it tells Pixi to install the exact dependency versions recorded in pixi.lock as opposed to searching for newer versions where possible. This ensures reproducibility, as updates to the dependencies in pixi.lock will only be pushed when they have been tested on Linux and MacOS systems by the core maintainer team.\n\n\n\nWhile Just isn‚Äôt as important as Pixi, I would still recommend installing it because of the conveniences it offers. With Just, the repo justfile provides a switchboard of command shorthands, including:\n\njust docs, which runs a series of Quarto commands to render and bundle the repo docs (including this file) as well as construct an updated readme\njust py, which lints and formats all the repo‚Äôs Python files.\njust docker, which builds and pushes a new version of the repo Docker Image.\njust env, which instantiates the Pixi environment.\njust all, which does everything (just doit will do the same thingüòé).\n\nRun just in the same directory as the repo justfile to list all available recipes, and check out the Just Programmer‚Äôs Manual for more about Just.\n\n\n\nIf you run pre-commit install in the repo root directory, it will install the pre-commit hooks in our .pre-commit-config.yaml. These hooks make sure that formatting throughout the repo files are consistent before they can be committed. Again, not essential, but nice!\n\n\n\nIf you don‚Äôt want to futz with all of the above or with setting up Dorado locally, you can also use the pipeline‚Äôs Docker Hub image nrminor/dorado-and-friends:v0.2.3 as a dev container. It should bring everything the pipeline and its dev environment needs, though its use as a dev container has not yet been tested.\n\n\n\nQuarto can be thought of as a renderer or compiler for documents written in supercharged Markdown. It can run code blocks, render to HTML, PDF, reveal.js presentations, Microsoft Powerpoint and Word documents, websites, books, and dozens of other formats. As such, I use Quarto markdown documents as a sort of ‚Äúur-format,‚Äù and render it out to other formats as desired. To render the project readme and any other .qmd documents in this project, you will need Quarto installed. The Quarto config file, _quarto.yml, controls project level settings, including a post-render section telling it to regenerate the Github-markdown-formatted readme from assets/index.qmd (via the project‚Äôs just readme recipe)."
  },
  {
    "objectID": "docs/developer.html#dorado",
    "href": "docs/developer.html#dorado",
    "title": "Developer Guide",
    "section": "Dorado",
    "text": "Dorado\nDorado is the one tricky dependency that isn‚Äôt handled by Pixi, as it is not published outside of the compiled executables in Oxford Nanopore‚Äôs GitHub Releases. To install it locally on your own system, you‚Äôll need to take additional care managing $PATH and downloading Dorado‚Äôs models yourself, as the Dockerfile does. Here‚Äôs how the Docker file handles the Linux version of Dorado it installs\nwget --quiet https://cdn.oxfordnanoportal.com/software/analysis/dorado-0.7.1-linux-x64.tar.gz && \\\ntar -xvf dorado-0.7.1-linux-x64.tar.gz && \\\nrm -rf dorado-0.7.1-linux-x64.tar.gz\n\nexport PATH=$PATH:$HOME/dorado-0.7.1-linux-x64/bin:$HOME/dorado-0.7.1-linux-x64/lib:$HOME/dorado-0.7.1-linux-x64\n\ndorado download\nThe process will be similar for MacOS users; we recommend consulting the above Dorado releases for the executable you should download. Feel free to raise an issue or a PR for more specific instructions in this section!"
  },
  {
    "objectID": "docs/developer.html#nextflow-organization",
    "href": "docs/developer.html#nextflow-organization",
    "title": "Developer Guide",
    "section": "Nextflow organization",
    "text": "Nextflow organization\nThis pipeline follows a slightly shaved-down project organization to the nf-core projects, which you can think of as a standardized means of organizing Nextflow .nf scripts. Like most Nextflow pipelines, the pipeline entrypoint is main.nf. main.nf performs some logic to call one of the workflow declaration scripts in the workflows/ directory. Workflow declaration scripts themselves call subworkflow declaration scripts in the subworkflows/ directory. Subworkflow scripts then call modules in the modules/ directory; these scripts actually do work, as they contain the individual processes that make up each workflow run.\nIf this seems like a lot, trust your instincts. Here‚Äôs why it‚Äôs worth it anyway:\n\nIt atomizes each building block for the pipeline, making it much easier to make the pipeline flexible to the inputs provided by the user. The pipeline doesn‚Äôt just run in one way; it can run in many ways. It also makes it possible to reuse the same processes at multiple stages of the pipeline, like we do with our reporting.nf module.\nIt makes it exceptionally easy to add functionality by plugging in new module scripts.\nIt also makes it exceptionally easy to switch out or reorder modules or bring in new subworkflows, e.g., a phylogenetics subworkflow. In other words, this project structure makes it easier for the maintainers to refactor.\nHaving defined individual workflow and module files here makes it easier to move around files and reuse Nextflow code for other workflows in the future. Instead of needing to scroll down a very long monolithic Nextflow script, just take the one file you need and get to work.\n\nLike most Nextflow pipelines, oneroof also has a few other important directories:\n\nbin/, which contains executable scripts that are called in various processes/modules.\nlib/, which contains Groovy utility functions that can be called natively within Nextflow scripts.\nconf/, which contains Nextflow configuration files with defaults, in our case, for the two workflow options (the Illumina workflow and the Nanopore workflow)."
  },
  {
    "objectID": "docs/developer.html#nextflow-configuration",
    "href": "docs/developer.html#nextflow-configuration",
    "title": "Developer Guide",
    "section": "Nextflow configuration",
    "text": "Nextflow configuration\noneroof comes with a large suite of parameters users can tweak to customize each run to their needs, in addition to profiles for controlling the environment the pipeline runs in. For now, we recommend users review the table in the repo README.md for documentation on the most commonly used parameters. That said, we expect to have more configuration documentation here soon."
  },
  {
    "objectID": "docs/developer.html#python-development",
    "href": "docs/developer.html#python-development",
    "title": "Developer Guide",
    "section": "Python development",
    "text": "Python development\nWe‚Äôre big fans of the Ruff linter and formatter, and we use it liberally in the writing of our Python scripts. To do so yourself in VSCode, we offer the following configuration for your User Settings JSON:\n{\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll.ruff\": \"explicit\",\n      \"source.organizeImports.ruff\": \"explicit\"\n    }\n  },\n  \"ruff.lineLength\": 88,\n  \"ruff.lint.select\": [\"ALL\"],\n  \"ruff.lint.ignore\": [\"D\", \"S101\", \"E501\", \"PTH123\", \"TD003\"],\n  \"ruff.nativeServer\": \"on\",\n  \"notebook.defaultFormatter\": \"charliermarsh.ruff\",\n}\nAt first, the lints may seem daunting, but virtually all lints come with persuasive documentation in the Ruff rule docs. Ultimately, our strict compliance with Ruff lints is inspired by the similar level of robustness that strict lints afford in the Rust ecosystem. We want our software to be resilient, performant, formatted in a familiar style, and reproducible in the long-term. Complying with as many lints as possible will only help, not harm, that end, even if it‚Äôs a bit annoying or overwhelming in the short-term."
  },
  {
    "objectID": "docs/pipeline_architecture.html",
    "href": "docs/pipeline_architecture.html",
    "title": "OneRoof Pipeline Architecture",
    "section": "",
    "text": "This document provides a comprehensive map of the OneRoof Nextflow pipeline structure, including workflow dependencies, data flow, and critical testing points."
  },
  {
    "objectID": "docs/pipeline_architecture.html#main-workflow-entry-point",
    "href": "docs/pipeline_architecture.html#main-workflow-entry-point",
    "title": "OneRoof Pipeline Architecture",
    "section": "1 Main Workflow Entry Point",
    "text": "1 Main Workflow Entry Point\n\n1.1 main.nf\n\nPurpose: Central orchestrator that routes to platform-specific workflows\nKey Functions:\n\nPlatform detection (Nanopore vs Illumina) based on input parameters\nInput channel initialization for all required files\nWorkflow selection and invocation\nEmail notification on completion\n\n\nInput Channels:\n\nch_primer_bed: Optional primer BED file\nch_refseq: Required reference FASTA\nch_ref_gbk: Optional GenBank file for annotation\nch_contam_fasta: Optional contamination sequences\nch_metagenomics_ref: Optional metagenomics reference\nch_snpeff_config: Optional SnpEff configuration\nch_primer_tsv: Optional primer TSV file\nch_sylph_tax_db: Optional Sylph taxonomy database"
  },
  {
    "objectID": "docs/pipeline_architecture.html#platform-specific-workflows",
    "href": "docs/pipeline_architecture.html#platform-specific-workflows",
    "title": "OneRoof Pipeline Architecture",
    "section": "2 Platform-Specific Workflows",
    "text": "2 Platform-Specific Workflows\n\n2.1 NANOPORE Workflow (workflows/nanopore.nf)\nWorkflow DAG:\ngraph TD\n    A[GATHER_NANOPORE] --&gt; B[PRIMER_HANDLING]\n    B --&gt; C[ALIGNMENT]\n    C --&gt; D[CONSENSUS]\n    C --&gt; E[VARIANTS]\n    C --&gt; F[HAPLOTYPING]\n    C --&gt; G[METAGENOMICS]\n    D --&gt; H[PHYLO]\n    E --&gt; I[SLACK_ALERT]\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n\n\n\n\n\n\nNote\n\n\n\nDashed boxes indicate optional workflow components\n\n\nKey Parameters:\n\nplatform = \"ont\"\nmin_variant_frequency = 0.2\nmin_qual = 10\n\n\n\n2.2 ILLUMINA Workflow (workflows/illumina.nf)\nWorkflow DAG:\ngraph TD\n    A[GATHER_ILLUMINA] --&gt; B[ILLUMINA_CORRECTION]\n    B --&gt; C[PRIMER_HANDLING]\n    C --&gt; D[ALIGNMENT]\n    D --&gt; E[CONSENSUS]\n    D --&gt; F[VARIANTS]\n    D --&gt; G[PHYLO]\n    D --&gt; H[METAGENOMICS]\n    E --&gt; I[SLACK_ALERT]\n    F --&gt; I\n\n    style C fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\nKey Parameters:\n\nplatform = \"illumina\"\nmin_variant_frequency = 0.05\nmin_qual = 20"
  },
  {
    "objectID": "docs/pipeline_architecture.html#sub-workflows-and-dependencies",
    "href": "docs/pipeline_architecture.html#sub-workflows-and-dependencies",
    "title": "OneRoof Pipeline Architecture",
    "section": "3 Sub-workflows and Dependencies",
    "text": "3 Sub-workflows and Dependencies\n\n3.1 Data Gathering Workflows\n\n3.1.1 GATHER_NANOPORE (subworkflows/gather_nanopore.nf)\nPurpose: Handle multiple Nanopore input formats\nInput Options:\n\nRemote POD5 monitoring (remote_pod5_location)\nLocal POD5 directory (pod5_dir)\nPre-called staging directory (precalled_staging)\nPre-processed data directory (prepped_data)\n\nProcess Flow:\ngraph LR\n    A[POD5 Input] --&gt; B[DOWNLOAD_MODELS]\n    B --&gt; C[BASECALL]\n    C --&gt; D[MERGE_BAMS]\n    D --&gt; E[DEMULTIPLEX]\n\n    F[Pre-called Input] --&gt; G[VALIDATE_NANOPORE]\n    E --&gt; G\n    G --&gt; H[FILTER_WITH_CHOPPER]\n    H --&gt; I[COMPRESS_TO_SORTED_FASTA]\n    I --&gt; J[FAIDX]\n    J --&gt; K[EARLY_RASUSA_READ_DOWNSAMPLING]\n\n\n3.1.2 GATHER_ILLUMINA (subworkflows/gather_illumina.nf)\nPurpose: Process paired-end Illumina FASTQ files\nProcess Flow:\ngraph LR\n    A[Paired FASTQs] --&gt; B[VALIDATE_ILLUMINA]\n    B --&gt; C[MERGE_READ_PAIRS]\n\n\n\n3.2 Processing Workflows\n\n3.2.1 ILLUMINA_CORRECTION (subworkflows/illumina_correction.nf)\nPurpose: Quality control and decontamination for Illumina reads\nProcess Flow:\ngraph TD\n    A[CORRECT_WITH_FASTP] --&gt; B[DECONTAMINATE]\n    B --&gt; C[FASTQC]\n    C --&gt; D[MULTIQC]\n    B --&gt; E[COMPRESS_TO_SORTED_FASTA]\n    E --&gt; F[FAIDX]\n    F --&gt; G[EARLY_RASUSA_READ_DOWNSAMPLING]\n\n    style B fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n\n\n3.2.2 PRIMER_HANDLING (subworkflows/primer_handling.nf)\nPurpose: Validate primers and extract complete amplicons\nInput Options:\n\nPrimer BED file\nPrimer TSV file\n\nProcess Flow:\ngraph TD\n    A[ORIENT_READS] --&gt; B[GET_PRIMER_PATTERNS]\n    B --&gt; C[FIND_COMPLETE_AMPLICONS]\n    B --&gt; D[TRIM_ENDS_TO_PRIMERS]\n    D --&gt; E[PER_AMPLICON_FILTERS]\n    E --&gt; F[MERGE_BY_SAMPLE]\n\n\n3.2.3 ALIGNMENT (subworkflows/alignment.nf)\nPurpose: Map reads to reference and generate coverage statistics\nProcess Flow:\ngraph TD\n    A[ALIGN_WITH_PRESET] --&gt; B[CONVERT_AND_SORT]\n    B --&gt; C[RASUSA_ALN_DOWNSAMPLING]\n    C --&gt; D[SORT_BAM]\n    D --&gt; E[INDEX]\n    E --&gt; F[MOSDEPTH]\n    F --&gt; G[PLOT_COVERAGE]\n    G --&gt; H[COVERAGE_SUMMARY]\n\n\n3.2.4 VARIANTS (subworkflows/variant_calling.nf)\nPurpose: Call and annotate variants\nProcess Flow:\ngraph TD\n    A[CALL_VARIANTS] --&gt; B[CONVERT_TO_VCF]\n    B --&gt; C[ANNOTATE_VCF]\n    C --&gt; D[EXTRACT_FIELDS]\n    D --&gt; E[MERGE_VCF_FILES]\n\n\n3.2.5 CONSENSUS (subworkflows/consensus_calling.nf)\nPurpose: Generate consensus sequences\nProcess Flow:\ngraph LR\n    A[CALL_CONSENSUS] --&gt; B[CONCAT]\n\n\n\n3.3 Optional Feature Workflows\n\n3.3.1 PHYLO (subworkflows/phylo.nf)\nPurpose: Phylogenetic analysis using Nextclade\nProcess Flow:\ngraph LR\n    A[CHECK_DATASET] --&gt; B[DOWNLOAD_DATASET]\n    B --&gt; C[RUN_NEXTCLADE]\n\n\n3.3.2 METAGENOMICS (subworkflows/metagenomics.nf)\nPurpose: Metagenomic classification using Sylph\nProcess Flow:\ngraph TD\n    A[SKETCH_DATABASE_KMERS] --&gt; C[CLASSIFY_SAMPLE]\n    B[SKETCH_SAMPLE_KMERS] --&gt; C\n    C --&gt; D[OVERLAY_TAXONOMY]\n    D --&gt; E[MERGE_TAXONOMY]\n\n    style D fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n    style E fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n\n\n3.3.3 HAPLOTYPING (subworkflows/haplotyping.nf)\nPurpose: Viral haplotype reconstruction (Nanopore only)\nCondition: Number of reference sequences equals number of amplicons"
  },
  {
    "objectID": "docs/pipeline_architecture.html#key-modulesprocesses",
    "href": "docs/pipeline_architecture.html#key-modulesprocesses",
    "title": "OneRoof Pipeline Architecture",
    "section": "4 Key Modules/Processes",
    "text": "4 Key Modules/Processes\n\n4.1 Critical Processes for Testing\n\ndorado.nfminimap2.nfivar.nfsamtools.nfvalidate.nf\n\n\n\nDOWNLOAD_MODELS: Model caching\nBASECALL: GPU-based basecalling\nDEMULTIPLEX: Barcode demultiplexing\n\n\n\n\nALIGN_WITH_PRESET: Platform-specific alignment\n\n\n\n\nCALL_VARIANTS: Variant detection\nCALL_CONSENSUS: Consensus generation\nCONVERT_TO_VCF: Format conversion\n\n\n\n\nCONVERT_AND_SORT: BAM processing\nINDEX: BAM indexing\n\n\n\n\nVALIDATE_NANOPORE: Input validation\nVALIDATE_ILLUMINA: Paired-end validation\nVALIDATE_PRIMER_BED: Primer validation"
  },
  {
    "objectID": "docs/pipeline_architecture.html#critical-testing-paths",
    "href": "docs/pipeline_architecture.html#critical-testing-paths",
    "title": "OneRoof Pipeline Architecture",
    "section": "5 Critical Testing Paths",
    "text": "5 Critical Testing Paths\n\n5.1 Minimal Test Path (No Primers)\n\nNanopore: POD5/FASTQ ‚Üí Basecall ‚Üí Align ‚Üí Consensus/Variants\nIllumina: Paired FASTQs ‚Üí Merge ‚Üí Align ‚Üí Consensus/Variants\n\n\n\n5.2 Full Test Path (With Primers)\n\nInput validation\nPrimer handling and amplicon extraction\nAlignment and coverage analysis\nVariant calling and annotation\nConsensus generation\nOptional: Phylogenetics, metagenomics, haplotyping\n\n\n\n5.3 Key Input Requirements\n\n\n\n\n\n\nMinimal Requirements\n\n\n\n\nReference FASTA (--refseq)\nSequencing data:\n\nNanopore: POD5 files + kit name OR pre-called BAM/FASTQ\nIllumina: Paired-end FASTQ directory\n\n\n\n\n\n\n\n\n\n\nFull Feature Requirements\n\n\n\n\nPrimer BED file (--primer_bed) or TSV (--primer_tsv)\nReference GenBank (--ref_gbk) for annotation\nSnpEff config for variant annotation\nContamination FASTA for decontamination\nMetagenomics database for classification"
  },
  {
    "objectID": "docs/pipeline_architecture.html#output-structure",
    "href": "docs/pipeline_architecture.html#output-structure",
    "title": "OneRoof Pipeline Architecture",
    "section": "6 Output Structure",
    "text": "6 Output Structure\n\n6.1 Nanopore Output Tree\nnanopore/\n‚îú‚îÄ‚îÄ 01_basecalled_demuxed/\n‚îú‚îÄ‚îÄ 02_primer_handling/\n‚îú‚îÄ‚îÄ 03_alignments/\n‚îú‚îÄ‚îÄ 04_consensus_seqs/\n‚îú‚îÄ‚îÄ 05_variants/\n‚îú‚îÄ‚îÄ 06_QC/\n‚îú‚îÄ‚îÄ 07_phylo/\n‚îú‚îÄ‚îÄ metagenomics/\n‚îî‚îÄ‚îÄ haplotyping/\n\n\n6.2 Illumina Output Tree\nillumina/\n‚îú‚îÄ‚îÄ 01_merged_reads/\n‚îú‚îÄ‚îÄ 02_primer_handling/\n‚îú‚îÄ‚îÄ 03_alignments/\n‚îú‚îÄ‚îÄ 04_consensus_seqs/\n‚îú‚îÄ‚îÄ 05_variants/\n‚îú‚îÄ‚îÄ 06_QC/\n‚îú‚îÄ‚îÄ 07_phylo/\n‚îî‚îÄ‚îÄ metagenomics/"
  },
  {
    "objectID": "docs/pipeline_architecture.html#configuration-and-parameters",
    "href": "docs/pipeline_architecture.html#configuration-and-parameters",
    "title": "OneRoof Pipeline Architecture",
    "section": "7 Configuration and Parameters",
    "text": "7 Configuration and Parameters\n\n7.1 Platform-Specific Defaults\n\n\n\nParameter\nNanopore\nIllumina\n\n\n\n\nmin_variant_frequency\n0.2\n0.05\n\n\nmin_qual\n10\n20\n\n\nAlignment preset\nmap-ont\nsr\n\n\n\n\n\n7.2 Resource Management\n\npod5_batch_size: Controls GPU memory usage\ndownsample_to: Coverage depth limiting\nbasecall_max: Parallel basecalling instances\nlow_memory: Resource-constrained mode\n\n\n\n7.3 Key Process Labels\n\nbig_mem: Memory-intensive processes (variant calling, consensus)\nGPU requirements: Dorado basecalling"
  },
  {
    "objectID": "docs/pipeline_architecture.html#error-handling-and-retry-strategy",
    "href": "docs/pipeline_architecture.html#error-handling-and-retry-strategy",
    "title": "OneRoof Pipeline Architecture",
    "section": "8 Error Handling and Retry Strategy",
    "text": "8 Error Handling and Retry Strategy\nMost processes implement:\nerrorStrategy { task.attempt &lt; 3 ? 'retry' : 'ignore' }\nmaxRetries 2\nThis provides resilience against transient failures while preventing infinite loops."
  },
  {
    "objectID": "docs/pipeline_architecture.html#testing-considerations",
    "href": "docs/pipeline_architecture.html#testing-considerations",
    "title": "OneRoof Pipeline Architecture",
    "section": "9 Testing Considerations",
    "text": "9 Testing Considerations\n\n9.1 Critical Validation Points\n\nInput file validation (exists, correct format)\nPrimer validation (coordinates, sequences)\nRead count filtering (empty file handling)\nPlatform-specific parameter application\nOptional workflow branching\n\n\n\n9.2 Edge Cases to Test\n\nEmpty input files\nNo reads passing filters\nMissing optional inputs\nPrimer mismatches\nLow coverage regions\nMultiple reference sequences\nRemote file watching timeout\n\n\n\n9.3 Integration Test Scenarios\n\nMinimal run: reference + reads only\nFull featured run: all optional inputs\nReal-time processing: file watching\nMulti-sample processing\nPlatform switching: same data, different platforms"
  }
]